# Data Science Training - Predictive Modeling

## Introduction
This project focuses on the ETL (Extract, Transform, Load) process within the realm of data science, specifically aimed at predictive modeling. The purpose is to develop a comprehensive understanding of how to manage data effectively, build predictive models, and utilize various machine learning techniques to forecast outcomes based on historical data. The importance of data management and data warehousing is highlighted throughout the project, as it serves as the foundation for effective data analysis and decision-making.

## Why is Data Warehouse Relevant?
Data warehousing is crucial for data engineers as it provides a centralized repository for storing and managing large volumes of data. This project offers hands-on experience in data integration, transformation, and loading processes, which are essential for building robust data pipelines. Understanding data warehousing enables data engineers to ensure data quality, accessibility, and reliability, which are vital for informed decision-making in any organization.

## Skills
Through this project, key skills have been developed, including:
- Data Integration
- Data Cleaning
- Database Management
- SQL Proficiency
- Data Analysis
- Predictive Modeling
- Machine Learning Techniques

## Exercises
### Exercise 01: Confusion Matrix
- **Description**: This exercise involves calculating and displaying a confusion matrix to evaluate the performance of a classification model. The focus is on understanding precision, recall, and F1-score metrics.
- **Relevance**: The confusion matrix is a fundamental tool for assessing the accuracy of classification models, providing insights into model performance and areas for improvement.

### Exercise 02: Heatmap
- **Description**: In this exercise, a heatmap is created to visualize the correlation coefficients between different features in the dataset.
- **Relevance**: Understanding feature correlations is essential for feature selection and engineering, helping to identify relationships that can enhance model performance.

### Exercise 03: Variances
- **Description**: This exercise involves calculating the variance of each skill and determining the number of components needed to reach 90% variance. A graph is displayed to represent the cumulative variances.
- **Relevance**: Analyzing variances helps in dimensionality reduction, ensuring that the most significant features are retained for model training.

### Exercise 04: Feature Selection
- **Description**: The goal of this exercise is to detect multicollinearity using the Variance Inflation Factor (VIF) and retain only the features with a VIF below 5.
- **Relevance**: Feature selection is critical for improving model interpretability and performance, as it reduces redundancy and enhances the predictive power of the model.

### Exercise 05: Decision Tree Classifier
- **Description**: A Decision Tree Classifier model is built to predict outcomes based on the training dataset. The model's performance is evaluated, and the decision tree is visualized.
- **Relevance**: Decision trees are intuitive models that provide clear insights into decision-making processes, making them valuable for both analysis and presentation.

### Exercise 06: KNN
- **Description**: This exercise involves implementing a K-Nearest Neighbors (KNN) algorithm to classify data points and evaluate its performance based on varying values of k.
- **Relevance**: KNN is a simple yet effective classification algorithm that helps in understanding the impact of hyperparameters on model performance.

## Conclusion
This project has provided valuable practical experience in data science, particularly in predictive modeling and data management. Key takeaways include the importance of data preprocessing, model evaluation, and the application of various machine learning techniques to solve real-world problems.

## License
This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.

## Author
üë§ **Andr√© Francisco Bai√£o Rol√£o C√¢ndido da Silva**
